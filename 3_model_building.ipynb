{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores building the **customer churn model** and an **interpretability model** to explain each prediction.\n",
    "In addition to making a prediction of whether a customer will churn, we will also be able to answer the question, \"why are they expected to churn?\"\n",
    "\n",
    "The following work will look fairly standard to anyone having trained machine learning models using python Jupyter notebooks.\n",
    "The CML platform provides a **fully capable Jupyter notebook environment** that data scientists know and love.\n",
    "\n",
    "If you haven't yet, run through the initialization steps in the README file and Part 1. \n",
    "In Part 1, the data is imported into the `default.telco_churn` table in Hive. All data accesses fetch from Hive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We again start by creating a `SparkSession` to fetch the data using Spark SQL, only this time we convert to a pandas `DataFrame` since we saw earlier that there are only 7k records in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_df = spark.sql(\"SELECT * FROM default.telco_churn\")\n",
    "spark_df.printSchema()\n",
    "df = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you don't have the Hive table, you can read the csv from the CML filesystem using pandas directly:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = '/home/cdsw'\n",
    "df = pd.read_csv(os.path.join(data_dir, 'raw', 'WA_Fn-UseC_-Telco-Customer-Churn-.csv'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic feature engineering\n",
    "\n",
    "\n",
    "Next we munge the data into appropriate types for later steps. \n",
    "In particular, we want to convert all the binary and string columns into pandas `Categorical` types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, glob, sys\n",
    "import dill  # a better pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = '/home/cdsw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idcol = 'customerID'  # ID column\n",
    "labelcol = 'Churn'  # label column\n",
    "cols = (('gender', True),  # (feature column, Categorical?)\n",
    "        ('SeniorCitizen', True),\n",
    "        ('Partner', True),\n",
    "        ('Dependents', True),\n",
    "        ('tenure', False),\n",
    "        ('PhoneService', True),\n",
    "        ('MultipleLines', True),\n",
    "        ('InternetService', True),\n",
    "        ('OnlineSecurity', True),\n",
    "        ('OnlineBackup', True),\n",
    "        ('DeviceProtection', True),\n",
    "        ('TechSupport', True),\n",
    "        ('StreamingTV', True),\n",
    "        ('StreamingMovies', True),\n",
    "        ('Contract', True),\n",
    "        ('PaperlessBilling', True),\n",
    "        ('PaymentMethod', True),\n",
    "        ('MonthlyCharges', False),\n",
    "        ('TotalCharges', False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(r'^\\s$', np.nan, regex=True).dropna().reset_index()  # drop blank rows\n",
    "df.index.name = 'id'  # name the index\n",
    "data, labels = df.drop(labelcol, axis=1), df[labelcol]  # separate out the labels\n",
    "data = data[[c for c, _ in cols]]  # only use the columns named in `cols`\n",
    "data = data.replace({'SeniorCitizen': {1: 'Yes', 0: 'No'}})  # Change 1/0 to Yes/No to match the other binary features\n",
    "\n",
    "# convert the categorical columns to pd.Categorical form\n",
    "for col, iscat in cols:\n",
    "    if iscat:\n",
    "        data[col] = pd.Categorical(data[col])\n",
    "labels = (labels == 'Yes')  # convert labels from str to bool\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning model\n",
    "\n",
    "This step follows a fairly standard ML workflow, which is to create a pipeline to:\n",
    "\n",
    "* Encode the categorical features as numeric\n",
    "* Normalize the numeric features\n",
    "* Train a classification model using these processed features\n",
    "\n",
    "We use *one-hot encoding*, *standardization*, and *logistic regression with cross-validation* for the three steps.\n",
    "Then we can evaluate the model's performance.\n",
    "\n",
    "Note: `CategoricalEncoder` and, later, `ExplainedModel` are helper classes pulled and edited from the original CFFL [interpretability report code](https://ff06-2020.fastforwardlabs.com/).\n",
    "You can inspect `churnexplainer.py` to see what they do under the hood.\n",
    "CML lets you continue to write modular code to keep things segregated and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from churnexplainer import CategoricalEncoder  # convert Categorical columns into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = CategoricalEncoder()\n",
    "X = ce.fit_transform(data)  # Categorical columns now have values 0 to num_categories-1\n",
    "y = labels.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cat_cols = list(ce.cat_columns_ix_.values())  # indices of the categorical columns (now numeric)\n",
    "ct = ColumnTransformer(\n",
    "    [('ohe', OneHotEncoder(), cat_cols)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "clf = LogisticRegressionCV(cv=5,solver='lbfgs', max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('ct', ct),  # 1. Encode the categorical features as numeric\n",
    "                 ('scaler', StandardScaler()),  # 2. Normalize the numeric features\n",
    "                 ('clf', clf)])  # 3. Train a classification model using these processed features\n",
    "pipe.fit(X_train, y_train)\n",
    "train_score = pipe.score(X_train, y_train)\n",
    "test_score = pipe.score(X_test, y_test)\n",
    "print(\"train\",train_score)\n",
    "print(\"test\", test_score)    \n",
    "print(classification_report(y_test, pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Random Forest\n",
    "Just for a comparison, lets compare this model to a Random Forest model.\n",
    "This is simpler since Random Forests do not need the categorical features encoded with a `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=100)\n",
    "pipe_rf = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('clf', clf_rf)])\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "train_score = pipe_rf.score(X_train, y_train)\n",
    "test_score = pipe_rf.score(X_test, y_test)\n",
    "print(\"train\",train_score)\n",
    "print(\"test\", test_score)\n",
    "print(classification_report(y_test, pipe_rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curve\n",
    "\n",
    "We can also generate an ROC Curve to visualize the model's performance and calculate the AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "logistic_regression_probabilities = pipe.predict_proba(X_test)\n",
    "logistic_regression_probabilities = logistic_regression_probabilities[:, 1]\n",
    "logistic_regression_auc = roc_auc_score(y_test, logistic_regression_probabilities)\n",
    "print('Logistic: AUROC=%.3f' % (logistic_regression_auc))\n",
    "logistic_regression_fpr, logistic_regression_tpr, _ = roc_curve(y_test, logistic_regression_probabilities)\n",
    "pyplot.plot(logistic_regression_fpr, logistic_regression_tpr, label='Logistic')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find an AUC of 0.83. Not bad for a quick exercise without fine tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability model\n",
    "We use [lime](https://github.com/marcotcr/lime) (Local Interpretable Model-Agnostic Explanations) to explain the predictions.\n",
    "It is a method of determining which feature has the greatest effect on the predicted value,\n",
    "and is explained in depth in the the [FFL report](https://ff06-2020.fastforwardlabs.com/).\n",
    "For more information, refer to the [lime documentation](https://lime-ml.readthedocs.io/en/latest/lime.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "data[labels.name + ' probability'] = pipe.predict_proba(X)[:, 1]\n",
    "\n",
    "# List of length number of features, containing names of features in order\n",
    "# in which they appear in X\n",
    "feature_names = list(ce.columns_)\n",
    "\n",
    "# List of indices of columns of X containing categorical features\n",
    "categorical_features = list(ce.cat_columns_ix_.values())\n",
    "\n",
    "# List of (index, [cat1, cat2...]) index-strings tuples, where each index\n",
    "# is that of a categorical column in X, and the list of strings are the\n",
    "# possible values it can take\n",
    "categorical_names = {i: ce.classes_[c]\n",
    "                     for c, i in ce.cat_columns_ix_.items()}\n",
    "class_names = ['No ' + labels.name, labels.name]\n",
    "explainer = LimeTabularExplainer(ce.transform(data),\n",
    "                                 feature_names=feature_names,\n",
    "                                 class_names=class_names,\n",
    "                                 categorical_features=categorical_features,\n",
    "                                 categorical_names=categorical_names)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining a Single Prediction\n",
    "\n",
    "Let's look at how one specfic prediction would be interpreted.\n",
    "Lime explains the prediction by giving every feature a weight from -1 to 1.\n",
    "Features with weights closer to -1 have a stronger impact in coming up with a 0 prediction result (will not churn) and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample().T  # reminder of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(ce.transform(data.sample())[0],pipe.predict_proba)\n",
    "for cols in exp.as_list():\n",
    "    print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that one of the features that contributes most strongly to the positive prediction is the short tenure of the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "Now that we've done all this work to build the models, we want to be able to use them later.\n",
    "The `ExplainedModel` class is a handy wrapper for using the `CategoricalEncoder`, the `Pipeline` object which *is* the churn model, and the Lime Explainer.\n",
    "Here, we use it to save these trained models for use in later parts of the Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churnexplainer import ExplainedModel\n",
    "explainedmodel = ExplainedModel(data=data, labels=labels, model_name='telco_linear',\n",
    "                                categoricalencoder=ce, pipeline=pipe,\n",
    "                                explainer=explainer,data_dir=data_dir)\n",
    "explainedmodel.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "We've now covered all the steps to **building a machine learning model** including interpretability\n",
    "and saved our work for use in later sections.\n",
    "\n",
    "In the next part of the series we will explore how to use the **Experiments** feature of CML\n",
    "for when we want to test lots of combinations of hyperparameters to fine tune our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
